---
title: "Querying a CDM"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

Many of the OHDSI tools are designed to use a database in the OMOP common data model format. OHDSI Studies and tools will send SQL to the database.



```{r}
library(DatabaseConnector)
connectionDetails <- createConnectionDetails(dbms = "redshift",
                                             user = "master",
                                             server = Sys.getenv("DB_SERVER"),
                                             password = Sys.getenv("DB_PASSWORD"),
                                             # password = keyring::key_get(), 
                                             # password = rstudioapi::askForPassword(),
                                             port = "5439")

conn <- connect(connectionDetails)
# disconnect(con) # use the disconnect function to disconnect

```

---
**A note about passwords**

Note that there are three commonly used options for passing database passwords to `DatabaseConnector`.

1. Save your password in your `.Renviron` file and then access it as an environment variable in your scripts.
2. Use the keyring package. Use `key_set()` to securely store your password and `key_get()` to retrieve it.
3. Interactively prompt for your password using `rstudioapi::askForPassword()` or `getPass::getPass()`

You should avoid typing your password into your analysis code files as plain text.
---

There are a few ways we can query the database from R.

**1) Using dbGetQuery() or similar R functions exported from the DatabaseConnector package**


```{r}
# send one query and get a result set
dbGetQuery(conn, "select * from synthea1k.concept limit 5")

# send one query and get result set
querySql(conn, sql = "select * from synthea1k.concept limit 2;")

# send multiple queries separated by semicolons
# note that executeSql does not return a result. It is used to create/modify tables in the database
executeSql(conn, "drop table if exists #x; create table #x (k INT); insert into #x values (2);")
querySql(conn, "select * from #x")

```


**2) Using a SQL code chunk in our notebook.**
Run one sql query and get a result

```{sql connection=conn}
select * from synthea1k.concept limit 5
```

Save result set to an R dataframe.

```{sql connection=conn, output.var = "df"}
select * from synthea1k.concept limit 5
```

```{r}
df
```


Note that if we make an error in our sql query we may need to send the rollback command to clear the transaction. This happens with postgres and redshift.





**3) Using dplyr (discussed later)**



# Get to know your schemas

In addition to knowing how to connect to the database we also want need to have three schemas available

**permissions**
1) cdmDatabaseSchema - the schema where your cdm lives (usually each cdm gets its own schema)
2) cohortDatabaseSchema - A schema where cohort tables can be created. You must have write access to this schema.
3) vocabDatabaseSchema - The schema where the vocabulary tables live (Usually same as the cdmDatabaseSchema)
4) resultsDatabaseSchema - Where Atlas and webapi store cohorts and pre-computed analyses (Achilles results).

We also need the name of the cdm database so each of these schemas is fully specified. 

`databaseName.schemaName.tableName`


To find these schemas we can look in a few places. 
- The rstudio connections pane (upper right)
- Querying the information_schema.tables table

```{sql, connection = conn}
select distinct table_schema from information_schema.tables
```

In our case, for the 1k person synthea dataset we have 

```{r}
cdmDatabaseSchema <- "mycdm.synthea1k"
cohortDatabaseSchema <- "mycdm.synthea1kresults"
vocabDatabaseSchema <- "mycdm.synthea1k"
resultsDatabaseSchema <- "mycdm.synthea1kresults"
```

Not all of these will be used for everything we do but we need to know what they are.



# Explore the CDM using SQL

We can get a long way toward understanding our data using SQL.

Some good places to look for example sql queries are

http://cdmqueries.omop.org/
https://data.ohdsi.org/QueryLibrary/


For example perhaps we want to know "what are the most prevalent concepts in the observation table?"

```{sql, connection = conn}
select concept_id, concept_name, count(*) as n 
from synthea1k.observation a
inner join synthea1k.concept b 
  on a.observation_concept_id = b.concept_id
group by concept_id, concept_name
order by n desc
limit 5;
```

# Rolling back a SQL transaction

When using postgres or redshift we might need to rollback a sql transaction if we have an error in our SQL code.

```{r, error=TRUE}
# a query with a typo
dbGetQuery(conn, "select * from synthea1k.concpt limit 1")

# subsequent queries might give an error like "commands ignored until end of transaction block"
dbGetQuery(conn, "select * from synthea1k.concept limit 1")

# rollback resets the transaction
dbExecute(conn, "rollback;")

# the error is gone
dbGetQuery(conn, "select * from synthea1k.concept limit 1")

```


# Achilles
In addition to running SQL queries we can use standardized analytic tools that query the database for us.

In this example we will run Achilles which is a powerful database characterization tool.




```{r, eval=F}

# create scratch schema and achilles results schema
username <- Sys.info()["user"]
renderTranslateExecuteSql(conn, 
                          sql = "create schema @scratchSchema; 
                          create schema @resultsSchema;",
                          scratchSchema = paste0("scratch_", username),
                          resultsSchema = paste0("results_", username))

scratchDatabaseSchema <- paste0("mycdm.scratch_", username)
resultsDatabaseSchema <- paste0("mycdm.results_", username)
```


```{r, warning=F}
achillesResults <- Achilles::achilles(connectionDetails, 
         cdmDatabaseSchema = "mycdm.synthea1k", 
         resultsDatabaseSchema = resultsDatabaseSchema,
         scratchDatabaseSchema = scratchDatabaseSchema,
         vocabDatabaseSchema = "mycdm.synthea1k",
         numThreads = 1,
         sourceName = "Synthea1k", 
         cdmVersion = "5.3",
         analysisIds = c(0,1), 
         runHeel = F,
         runCostAnalysis = F)

```

We can take a look at the tables that were created.
```{r}
DatabaseConnector::getTableNames(conn, resultsDatabaseSchema)
```



Once the results schema is added to the configuration section of Atlas the Achilles results can be viewed in Atlas under the "Data Sources".

# Data Quality Dashboard

Another tool that is helpful to run to check that your CDM is ready for prime time is DataQualityDashboard. As a researcher or data scientist you are probably not the person who created the CDM you want to use. How can you gain confidence that the CDM conforms to the correct specification and that the standardized analytics will work correctly?


```{r}
# check that all cdm fields are present
dqChecks <- DataQualityDashboard::executeDqChecks(connectionDetails = connectionDetails,
                                                  cdmDatabaseSchema = "mycdm.synthea1k", 
                                                  resultsDatabaseSchema = "mycdm.synthea1kresults",
                                                  writeToTable = F,
                                                  cdmSourceName = "Synthea1k",
                                                  outputFolder = "~/synthea1kDQChecks", 
                                                  checkNames = "cdmField")

# which fields are missing?
library(dplyr)
dqChecks %>% 
  filter(FAILED == 1) %>% 
  select(CDM_TABLE_NAME, CDM_FIELD_NAME)

```

We can also take a look at the log that was created.
```{r, eval=FALSE}
file.edit("~/synthea1kDQChecks/Synthea1k/log_DqDashboard_Synthea1k.txt")
```

or view the results in the DQ Dashboard.

```{r, eval=FALSE}
DataQualityDashboard::viewDqDashboard("~/synthea1kDQChecks/Synthea1k/results_Synthea1k.json")
```


It looks like we are missing a lot from the visit detail table. Let's confirm this.
```{r}
df <- dbGetQuery(conn, "select * from synthea1k.VISIT_DETAIL limit 5")
nrow(df)
colnames(df)
```

We have visit_detail.visit_start_date instead of visit_detail.visit_detail_start_date
This is good information to know before we try to run standardized analytics.

```{r}
df <- dbGetQuery(conn, "select * from synthea1k.COST limit 5")
nrow(df)
colnames(df)
```

In the case of the cost table it looks like we have a typo in the name "reveue_code_source_value". These types of checks can help avoid errors later on when using OHDSI tools.


Another way to run long-running scripts from R is to use "Source as local job". If you like try running all of the data quality checks by sourcing the "run_all_dqd_checks.R" as a local job.

Read about dataQualityDashboard options at https://ohdsi.github.io/DataQualityDashboard/articles/CheckTypeDescriptions.html



# Using dplyr for queries

dplyr is a popular data manipulation tool in R. We can also use dplyr to query the cdm but we will need to connect using the DBI::dbConnect function.

```{r}
library(dplyr)
library(RPostgreSQL)
host <- stringr::str_remove(Sys.getenv("DB_SERVER"), "/mycdm")

con <- DBI::dbConnect(drv = dbDriver("PostgreSQL"),
                      host = host,
                      dbname = "mycdm",
                      user = "master",
                      password = Sys.getenv("DB_PASSWORD"),
                      port = "5439")



condition <- tbl(con, dbplyr::in_schema("synthea1k", "condition_occurrence"))
concept <- tbl(con, dbplyr::in_schema("synthea1k", "concept"))

# What are the most frequently occuring condition concepts?
condition %>% 
  inner_join(concept, by = c("condition_concept_id" = "concept_id")) %>% 
  count(condition_concept_id, concept_name, sort = T)

```


# Parameterized SQL & SQL translation

In OHDSI we frequently want to run SQL on multiple databases with only slight modifications. Rather than copying the SQL code we can use parameters to abstract away the differences. The SqlRender package provides the functionality to write parameterized SQL as well as translate SQL from the OHDSI SQL dialect (a subset of SQL Server's T-SQL) to a variety of other database dialects automatically. (Postgres, Big Query, Impala, Oracle, ect.)



```{r}
library(SqlRender)

# We can paramaterize SQL
sql <- "SELECT * FROM @x WHERE person_id = @a;"
render(sql, x = "observation", a = 123)

# And translate it to different database dialects
sql <- "SELECT TOP 10 * FROM person;"
translate(sql, targetDialect = "postgresql")

```

Suppose we want to execute a query created by someone in the OHDSI community on our database. We can use SqlRender to accomplish this. Let's finish up this section by executing a modified version of the query in The Book of OHDSI https://ohdsi.github.io/TheBookOfOhdsi/SqlAndR.html#implementing-the-study-using-sql-and-r

Instead of a drug exposure-condition outcome rate let's look at the incidence of amputation among patients with type 2 diabetes.

```{r}
# create the parameters we will use for sql queries
cohortDatabaseSchema <- scratchDatabaseSchema
cohortTable <- "cohort"
cdmDatabaseSchema <- "mycdm.synthea23m"
```


```{r}
# create an empty cohort table
renderTranslateExecuteSql(conn, 
                          "DROP TABLE IF EXISTS @cohort_db_schema.@cohort_table;",
                          cohort_db_schema = cohortDatabaseSchema,
                          cohort_table = cohortTable)

sql <- "
CREATE TABLE @cohort_db_schema.@cohort_table (
  cohort_definition_id INT,
  cohort_start_date DATE,
  cohort_end_date DATE,
  subject_id BIGINT
);
"
renderTranslateExecuteSql(conn, sql,
                          cohort_db_schema = cohortDatabaseSchema,
                          cohort_table = cohortTable)
```

The CohortDiagnostics package contains a convenience function for doing this as well.
```{r, eval=F}

# This function will overwrite an existing cohort table of the same name!
CohortDiagnostics::createCohortTable(connection = conn,
                                     cohortDatabaseSchema = cohortDatabaseSchema,
                                     cohortTable = cohortTable)
```

**Create exposure cohort**
Cohort entry will be first occurrence of type 2 diabetes mellitus and cohort exit will be the end of continuous observation.

```{r}

sql <- "
INSERT INTO @cohort_db_schema.@cohort_table (
  cohort_definition_id,
  cohort_start_date,
  cohort_end_date,
  subject_id
)
SELECT 
  1 AS cohort_definition_id,
  cohort_start_date,
  observation_period_end_date as cohort_end_date,
  subject_id
FROM (
  SELECT 
    condition_start_date AS cohort_start_date,
    person_id AS subject_id
  FROM (
    SELECT 
      condition_start_date,
      condition_end_date,
      person_id,
      ROW_NUMBER() OVER (
        PARTITION BY person_id
            ORDER BY condition_start_date
      ) order_nr
    FROM @cdm_db_schema.condition_occurrence
    WHERE condition_concept_id =	201826 -- Type 2 Diabetes Mellitus
  ) ordered_exposures
  WHERE order_nr = 1
) first_occurrence
INNER JOIN @cdm_db_schema.observation_period
  ON subject_id = person_id
    AND observation_period_start_date < cohort_start_date
    AND observation_period_end_date > cohort_start_date
"




renderTranslateExecuteSql(conn, sql,
                          cohort_db_schema = cohortDatabaseSchema,
                          cohort_table = cohortTable,
                          cdm_db_schema = cdmDatabaseSchema)

```

**Create outcome cohort**
Cohort entry will be on the day of a leg or foot amputation. Cohort exit will be the same day as the procedure.

```{r}
sql <- "
INSERT INTO @cohort_db_schema.@cohort_table (
 cohort_definition_id,
 cohort_start_date,
 cohort_end_date,
subject_id
)
SELECT 
  2 AS cohort_definition_id,
  cohort_start_date,
  cohort_end_date,
  subject_id
FROM (
  SELECT DISTINCT 
    person_id AS subject_id,
    procedure_date AS cohort_start_date,
    procedure_date AS cohort_end_date
  FROM @cdm_db_schema.procedure_occurrence
  WHERE procedure_concept_id IN(4195136, 4108565) -- Amputation of foot or above knee
) distinct_occurrence
"

renderTranslateExecuteSql(conn, sql,
                          cohort_db_schema = cohortDatabaseSchema,
                          cohort_table = cohortTable,
                          cdm_db_schema = cdmDatabaseSchema)


```

```{sql connection=conn}
 SELECT DISTINCT 
    person_id AS subject_id,
    procedure_date AS cohort_start_date,
    procedure_date AS cohort_end_date
  FROM mycdm.synthea100k.procedure_occurrence
  WHERE procedure_concept_id = 4195136 or procedure_concept_id =  4108565
```

```{r}
# check to see how many patients are in the cohort
# glue is a nice package for building queries too. Anything inside {braces} is evaluated as R code.
dbGetQuery(conn, glue::glue("
select 
  cohort_definition_id, 
  count(*) as count 
from {cohortDatabaseSchema}.{cohortTable}
group by cohort_definition_id"))
```

**Calculate the incidence rate**

```{r}
sql <- "
WITH tar AS (
  SELECT concept_name AS gender,
    FLOOR((YEAR(cohort_start_date) -
          year_of_birth) / 10)*10 AS age,
    subject_id,
    cohort_start_date,
    CASE WHEN DATEADD(DAY, 3650, cohort_start_date) >
      observation_period_end_date
    THEN observation_period_end_date
    ELSE DATEADD(DAY, 3650, cohort_start_date)
    END AS cohort_end_date
  FROM @cohort_db_schema.@cohort_table
  INNER JOIN @cdm_db_schema.observation_period
    ON subject_id = observation_period.person_id
      AND observation_period_start_date < cohort_start_date
      AND observation_period_end_date > cohort_start_date
  INNER JOIN @cdm_db_schema.person
    ON subject_id = person.person_id
  INNER JOIN @cdm_db_schema.concept
    ON gender_concept_id = concept_id
  WHERE cohort_definition_id = 1 -- Exposure
)
SELECT days.gender,
    days.age,
    days,
    CASE WHEN events IS NULL THEN 0 ELSE events END AS events
FROM (
  SELECT gender,
    age,
    SUM(DATEDIFF(DAY, cohort_start_date,
      cohort_end_date)) AS days
  FROM tar
  GROUP BY gender,
    age
) days
LEFT JOIN (
  SELECT gender,
      age,
      COUNT(*) AS events
  FROM tar
  INNER JOIN @cohort_db_schema.@cohort_table amputation
    ON tar.subject_id = amputation.subject_id
      AND tar.cohort_start_date <= amputation.cohort_start_date
      AND tar.cohort_end_date >= amputation.cohort_start_date
  WHERE cohort_definition_id = 2 -- Outcome
  GROUP BY gender,
    age
) events
ON days.gender = events.gender
  AND days.age = events.age;
"

results <- renderTranslateQuerySql(conn, sql,
                                   cohort_db_schema = cohortDatabaseSchema,
                                   cohort_table = cohortTable,
                                   cdm_db_schema = cdmDatabaseSchema,
                                   snakeCaseToCamelCase = TRUE)
```


```{r}
# Compute incidence rate (IR) :
results$ir <- 1000 * results$events / results$days / 365

library(ggplot2)
ggplot(results, aes(x = age, y = ir, group = gender, color = gender)) +
  geom_line() +
  xlab("Age") +
  ylab("Incidence (per 1,000 patient years)") +
  ggtitle("Incidence of leg amputation in type 2 diabetes patients")
```


Notice how we used almost the same SQL code from the [The Book of OHDSI](https://ohdsi.github.io/TheBookOfOhdsi/SqlAndR.html#implementing-the-study-using-sql-and-r) but made some slight modifications by changing the concept IDs? This is ability to write generic SQL code that will run a type of analysis against any database in the OMOP CDM format is a foundational principle that many OHDSI tools are built on.

# Exercise

Head over to the [OHDSI Query Library](https://data.ohdsi.org/QueryLibrary/). Find a query that looks interesting and try to run it on the Synthea data. Use the data characterization reports in Atlas to identify what conditions/drugs/procedures are captured in your CDM. Share your query and results on in the Teams chat.


Always remember to disconnect from the database when you are done using it.

```{r}
DatabaseConnector::disconnect(conn)
DBI::dbDisconnect(con)
```

